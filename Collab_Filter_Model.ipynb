{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import implicit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "le = pd.read_csv('nprs_final.csv',usecols=['user_id','track_id','created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "les = le.sort_values(by='created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign each event a count of 1\n",
    "les['count'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by user/item id and sum counts\n",
    "rating = les.groupby(['user_id','track_id'],as_index=False)['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique list of users\n",
    "users = list(rating.user_id.unique())\n",
    "#unique list of songs\n",
    "songs = list(rating.track_id.unique())\n",
    "#count of interactions per user/item\n",
    "conf = list(rating['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign sequential user and item indicies for sparse matrix\n",
    "#assign category codes to user\n",
    "cols = rating.user_id.astype('category').cat.codes\n",
    "#assign category code to item\n",
    "rows = rating.track_id.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create sparse matrix of users/items with interaction count as entry\n",
    "smat = sp.csr_matrix((conf,(rows,cols)),shape=(len(songs),len(users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<81343x21220 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 1053887 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of the user/item matrix is 99.93894398121415\n"
     ]
    }
   ],
   "source": [
    "matrix_size = smat.shape[0]*smat.shape[1] # Number of possible interactions in the matrix\n",
    "num_listens = len(smat.nonzero()[0]) # Number of items interacted with\n",
    "sparsity = 100*(1 - (num_listens/matrix_size))\n",
    "print(f'Sparsity of the user/item matrix is {sparsity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import implicit library als, eval metric, and split function\n",
    "from implicit.evaluation import precision_at_k, train_test_split\n",
    "from implicit.als import AlternatingLeastSquares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train and test set\n",
    "train, test = train_test_split(smat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intel MKL BLAS detected. Its highly recommend to set the environment variable 'export MKL_NUM_THREADS=1' to disable its internal multithreading\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f35eef282a1405ab924631b94a20eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#instantiate and fit model\n",
    "model = AlternatingLeastSquares(factors=130, regularization=20, iterations=20)\n",
    "model.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation: Precision @ k\n",
    "\n",
    "Due to not having access to the song/artist names, we are limited to a qunatitative accessment of how well the model is performing.  For this metric we choose. precision at k. P @ k is chosen because it is a good indicator to how well the model is performing, and it is an intuitive metric.  Precision at k is the proportion of recommended items in the top-k set that are relevant.  If we set k = 10, then 3 relevant items in the top k recommendations would yield precision at k of 30%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c650c46d8f04e76a2781083cf6c9d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21220.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ALS_precision_at_k = precision_at_k(model, train.T.tocsr(), test.T.tocsr(), K=10, num_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06839781561453441"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALS_precision_at_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ALS model, we get a precision at k of .68%.  That means that less than 1 item in out top ten recommendations we relevant to the user.  Obviously, we would like to so better than that.  Generally speaking, when I qualitatively evaluate my recommended songs on Spotify, I would say that precision at k would fall somewhere between 30 - 40%.  However, the Spotify model is a sophisticated hybrid model, so one would expect good p@k scores. \n",
    "\n",
    "The low p@k score for our model is most likely due to having a very sparse matix which is a characteristic of the cold start problem.\n",
    "\n",
    "Another possible way to evaulate the model would be to get recommendations for a specific user, and then get recommendations of a similar user.  We would then compare the lists and see if any recommended items appear for each user.  We can do this by making lists of the recommended items and looking at the intersection of those lists.  We can extend that by taking any items that appear in both lists, and finding similar items.  We could then take that list and cross-reference to see if any similar items appears in the recommendations for the user/similar user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recommend items for user 937\n",
    "recs937 = model.recommend(937, smat)\n",
    "\n",
    "#grab items and store in list\n",
    "reclist_937 = [item for item, _ in recs937]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(937, 0.045337625),\n",
       " (13543, 0.02613195),\n",
       " (20334, 0.02596296),\n",
       " (17891, 0.025756504),\n",
       " (6987, 0.025645282),\n",
       " (3085, 0.024724733),\n",
       " (737, 0.02455601),\n",
       " (3368, 0.024459321),\n",
       " (16248, 0.024253428),\n",
       " (12122, 0.023699267)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find similar users to user 937\n",
    "model.similar_users(937)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recommend song to user closest to 937\n",
    "similar_user_items = model.recommend(13543,smat)\n",
    "\n",
    "#grab items recommended to similar user\n",
    "reclist_similar_user =  [item for item, _ in similar_user_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{50405, 55849}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if any items overlap similar user\n",
    "set(reclist_similar_user).intersection(set(reclist_937))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if similar items are in user 937 or similar users rec list\n",
    "sim_songs = model.similar_items(55489)\n",
    "sim_song_list = [x for x, _ in sim_songs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(sim_song_list).intersection(set(reclist_937))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(sim_song_list).intersection(set(reclist_similar_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that songs 50405 and 55849 both appears in user 937's recommendation list and that users most similar user (user 13543) recommendation list.  However, when we find items most similar to song 55489, none of the returned items appear in the recommendation list for either user.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pseudo_bounce\\anaconda3\\lib\\site-packages\\lightfm\\_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn('LightFM was compiled without OpenMP support. '\n"
     ]
    }
   ],
   "source": [
    "# import lightfm library and auc_score function\n",
    "from lightfm import LightFM # model\n",
    "from lightfm.evaluation import precision_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate light fm model with 130 components (same as als model)\n",
    "# se 'loss' equal to 'warp' to optimize precision at k\n",
    "modelfm = LightFM(\n",
    "    no_components=130,\n",
    "    learning_rate=0.05,\n",
    "    loss='warp',\n",
    "    random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lightfm dataset class\n",
    "from lightfm.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit dataset class with user/items while supplying no content features\n",
    "dataset = Dataset()\n",
    "dataset.fit(\n",
    "    users, \n",
    "    songs,\n",
    "    item_features=None, \n",
    "    user_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x2422a201888>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model on the training data\n",
    "modelfm.fit(\n",
    "    train,\n",
    "    item_features=None,\n",
    "    user_features=None, sample_weight=None,\n",
    "    epochs=5, num_threads=4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score the model on the test data\n",
    "test_score = precision_at_k( \n",
    "        modelfm, test, \n",
    "        item_features=None, \n",
    "        user_features=None, \n",
    "        num_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.039319575"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lightfm documentation states that any user/item pair with no interaction is set to zero, which reflects in the test_score mean.  We need to remove the zero values and recalculate the get the average precision at k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import counter to create dictionary of values\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_dict = Counter(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 36492,\n",
       "         0.2: 2769,\n",
       "         0.1: 13104,\n",
       "         0.3: 560,\n",
       "         0.4: 112,\n",
       "         0.5: 17,\n",
       "         0.6: 1})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12594940742292332\n"
     ]
    }
   ],
   "source": [
    "# we see that 36492 user/item pairs had no interaction\n",
    "#docs state that indices with zero interaction get a score of zero, remove the zeros and calculate average\n",
    "test_no_zero = [x for x in test_score if x != 0.0]\n",
    "\n",
    "#calculate mean test p@k\n",
    "mean_test_pak = sum(test_no_zero) / len(test_no_zero)\n",
    "\n",
    "print(mean_test_pak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate train precision @ k for comparison\n",
    "train_score = precision_at_k( \n",
    "        modelfm, train, \n",
    "        item_features=None, \n",
    "        user_features=None, \n",
    "        num_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create count dictionary\n",
    "train_score_dict = Counter(train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.1: 27993,\n",
       "         0.0: 18947,\n",
       "         0.8: 335,\n",
       "         0.2: 15161,\n",
       "         0.3: 7529,\n",
       "         0.4: 3949,\n",
       "         0.6: 1311,\n",
       "         0.7: 727,\n",
       "         0.5: 2333,\n",
       "         1.0: 25,\n",
       "         0.9: 107})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21054481628080146\n"
     ]
    }
   ],
   "source": [
    "#remove zeros from list\n",
    "train_no_zero = [x for x in train_score if x != 0.0]\n",
    "#calculate train average precision @ k\n",
    "mean_train_pak = sum(train_no_zero) / len(train_no_zero)\n",
    "\n",
    "print(mean_train_pak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05755159180838891"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " mean_test_pak-ALS_precision_at_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do see an improvement of 0.057 in the precision @ k when compared to the ALS model.  The lightfm model yielded a score of 0.12.  It is obvious both models suffer from a spare user/item matrix and the cold start problem.  This can be addressed by creating a hybrid system the incorporates meta-data from the users and items.  Lightfm was chosen because of its ability to incorporate these features.  However, after spending considerable time attempting to build the hybrid model without success, the task is left for a future iteration of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
